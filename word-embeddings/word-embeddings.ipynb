{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pralay-ahluwalia/nlp/blob/main/word-embeddings/word-embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai_7ZFUnBmN5",
        "outputId": "437421de-3ae2-486e-e352-dcb054271a71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Only needed in colab\n",
        "!pip install gensim\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this worbook I try to demonstrate how word embeddings can differ based on the training data used to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b61b4b378784449be5bf4d59ad7e5b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)80420_win10_100d.txt:   0%|          | 0.00/3.48G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gensim \n",
        "from gensim.models import KeyedVectors\n",
        "from huggingface_hub import hf_hub_download\n",
        "model = KeyedVectors.load_word2vec_format(hf_hub_download(repo_id=\"Word2vec/wikipedia2vec_enwiki_20180420_win10_100d\", filename=\"enwiki_20180420_win10_100d.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('tripoint', 0.7618456482887268),\n",
              " ('borders', 0.7544373273849487),\n",
              " ('straddling', 0.7415540218353271),\n",
              " ('bajakovo', 0.7244082093238831),\n",
              " ('roundtime', 0.722857654094696),\n",
              " ('crossing', 0.7226234078407288),\n",
              " ('holmec', 0.722411572933197),\n",
              " ('bordering', 0.7200196385383606),\n",
              " ('statesmexico', 0.7150804996490479),\n",
              " ('demarcating', 0.7092791199684143)]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar(\"border\")\n",
        "#model.most_similar(\"guitar\")\n",
        "#model.get_vector(\"parent\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load some local data\n",
        "mypath = \"/home/pralay/workspace/data/text/\"\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "\n",
        "list_texts = []\n",
        "for fnm in onlyfiles:\n",
        "    f = open(mypath + fnm, \"r\" )\n",
        "    list_texts.append(f.read())\n",
        "    f.close()\n",
        "\n",
        "#We now train on local data. We should see the difference in the neighbours.\n",
        "list_texts = [ gensim.utils.simple_preprocess(doc) for doc in list_texts ]\n",
        "list_texts = [ gensim.parsing.preprocessing.remove_stopword_tokens(doc) for doc in list_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Locally trained Word2Vec\n",
        "model_lcl = gensim.models.Word2Vec(list_texts, min_count = 1,\n",
        "                              vector_size = 100, window = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('disengagement', 0.6141754388809204),\n",
              " ('boundary', 0.5988191366195679),\n",
              " ('departmental', 0.5627535581588745),\n",
              " ('sectional', 0.5558925271034241),\n",
              " ('disputed', 0.5484164953231812),\n",
              " ('pangong', 0.5478925704956055),\n",
              " ('crossing', 0.5460109114646912),\n",
              " ('galwan', 0.5430428385734558),\n",
              " ('flow', 0.5213136672973633),\n",
              " ('demarcation', 0.5181994438171387)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#vector = model_lcl.wv['peace']  # get numpy vector of a word\n",
        "#\"guitar\"\n",
        "#\"parent\"\n",
        "model_lcl.wv.most_similar('border', topn=10)  # get other similar words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNYSGawyX0YQHeWuQz32v09",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('.venv': poetry)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ccc2e760ead2fbaf43c431813b454d0ecac69a1f06ba75b6e1afa154f604e861"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
